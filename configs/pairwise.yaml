# Model hyperparameters
learning_rate: 0.001 # The learning rate for the optimizer
batch_size: 8        # Number of samples per batch
test_batch_size: 8        # Number of samples per batch
epochs: 1            # Total training epochs
#optimizer: "ranger"       # Optimization algorithm
dropout: 0.1     # Dropout regularization rate
weight_decay: 0.0001
k: 5 #not used
ninp: 384
nlayers: 48
decoder_ninp: 256
decoder_nhead: 8
decoder_nlayers: 3
nclass: 8
#nexperiments: 5
ntoken: 6 #AUGC + padding/N token
nhead: 12
#use_bpp: False
use_flip_aug: false
#bpp_file_folder: "../../input/bpp_files/"
gradient_accumulation_steps: 1
use_triangular_attention: false
pairwise_dimension: 128
dim_msa: 32
clip_grad_norm: 1.0
max_len: 177
max_seq: 128
log_interval: 2000
use_gradient_checkpoint: true
warmup_steps: 200

#Data scaling
use_data_percentage: 1
use_dirty_data: true # turn off for data scaling and data dropout experiments

#Loss weights
#loss = raw_read_loss + binary_loss + outer_product_loss + r_norm_loss
raw_read_loss_weight: 1
binary_loss_weight: 1
outer_product_loss_weight: 0.2
r_norm_loss_weight: 1


# Other configurations
fold: 0
nfolds: 6
input_dir: "/lustre/fs0/scratch/shared/Ribonanza2/"
gpu_id: "0"
previous_checkpoint: "../../exps/test41_biglr/models/epoch_14/pytorch_model_fsdp.bin"
#previous_checkpoint: "none"

# data files
hdf_files:
  - Ribonanza2A_Genscript.v0.1.0.hdf5
  - Ribonanza2B_full40B.v0.1.0.hdf5
  - Ribonanza2C_full40B.v0.1.0.hdf5
  - Ribonanza2D.v0.1.0.hdf5
  - Ribonanza2E.v0.1.0.hdf5

raw_read_files:
  - - "A/RTB008_GenScript_DMS.memmap"
    - "A/RTB010_GenScript_2A3.memmap"
  - - "B/RTB004_Marathon_DMS.memmap"
    - "B/RTB006_SSII_2A3.memmap"
  - - "C/RTB000_Marathon_DMS.memmap"
    - "C/RTB002_SSII_2A3.memmap"
  - - "D/RTB000_Marathon_DMS.memmap"
    - "D/RTB002_SSII_2A3.memmap"
  - - "E/RTB000_Marathon_DMS.memmap"
    - "E/RTB002_SSII_2A3.memmap"

sublib_csv_files: 
  - "sublib_id.csv" # if exists do stratified kfold else do random kfold
  - "none"
  - "none"
  - "none"
  - "none"

